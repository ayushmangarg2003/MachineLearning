{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPBwGFB4hCf8G24Y5m1sJmf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ayushmangarg2003/MachineLearning/blob/main/parrotparaphraser.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "OL7RV5Ag2p5l"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import T5ForConditionalGeneration, T5Tokenizer\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def paraphrase_text(input_text, model_name=\"prithivida/parrot_paraphraser_on_T5\", max_length=1024, num_beams=5):\n",
        "    \"\"\"\n",
        "    Paraphrase AI-generated text to make it sound more human-like.\n",
        "\n",
        "    Args:\n",
        "        input_text (str): The AI-generated text to paraphrase.\n",
        "        model_name (str): Hugging Face model name for paraphrasing.\n",
        "        max_length (int): Maximum length of the output text.\n",
        "        num_beams (int): Number of beams for beam search (controls diversity).\n",
        "\n",
        "    Returns:\n",
        "        str: Paraphrased human-like text.\n",
        "    \"\"\"\n",
        "    # Load pre-trained T5 model and tokenizer\n",
        "    try:\n",
        "        tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "        model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading model: {e}\")\n",
        "        return None\n",
        "\n",
        "    # Move model to GPU if available\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = model.to(device)\n",
        "\n",
        "    # Prepare input text with paraphrase prefix\n",
        "    input_text = f\"paraphrase: {input_text}\"\n",
        "    inputs = tokenizer.encode(input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
        "    inputs = inputs.to(device)\n",
        "\n",
        "    # Generate paraphrased text\n",
        "    outputs = model.generate(\n",
        "        inputs,\n",
        "        max_length=max_length,\n",
        "        num_beams=num_beams,\n",
        "        early_stopping=True,\n",
        "        no_repeat_ngram_size=2\n",
        "    )\n",
        "\n",
        "    # Decode and clean output\n",
        "    paraphrased_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return paraphrased_text"
      ],
      "metadata": {
        "id": "M2z1F0Xe2t8a"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    # Example AI-generated text\n",
        "    ai_text = '''Introduction\n",
        "In the ever-evolving world of artificial intelligence, model performance isn’t just about size or data—it’s about context. What is MCP, or Model Context Protocol? It’s a concept that’s quietly redefining how AI systems understand and process information. As generative AI tools like ChatGPT, Claude, and Gemini become central to work and life, protocols like MCP are crucial for better memory, context retention, and long-form reasoning.\n",
        "\n",
        "Understanding what is MCP is important for developers, researchers, and tech-savvy users who want more control over how large language models (LLMs) interact with historical data, user profiles, and live inputs. This article offers a complete breakdown of MCP explained, including its structure, applications, and what it means for the future of AI.\n",
        "\n",
        "What is MCP?\n",
        "Model Context Protocol (MCP) is a standardized way to provide structured, scoped, and persistent context to large language models (LLMs). In simple terms, MCP defines how different pieces of information (like user data, conversation history, and app-specific context) are fed to AI models in a reliable, interpretable, and privacy-aware format.\n",
        "\n",
        "Instead of relying solely on long prompts, which can be expensive and error-prone, MCP allows developers to build smarter, more memory-capable AI applications by passing contextual data in a clean, consistent structure.\n",
        "\n",
        "Think of it as a communication standard between your app and the AI model—ensuring it knows what matters, remembers past interactions, and responds with greater relevance.\n",
        "\n",
        "Historical Background\n",
        "The concept of model context has been around since early LLMs like GPT-2. However, it was limited by token constraints and lacked persistence. MCP emerged as a solution in the context of more advanced AI agents and applications needing:\n",
        "\n",
        "Long-term memory\n",
        "\n",
        "User-specific customization\n",
        "\n",
        "Multi-modal, multi-source context support\n",
        "\n",
        "OpenAI introduced MCP in 2024 as part of their push toward more agentic models. Its early adopters include tools like ChatGPT with custom instructions and memory features, and developer platforms looking to integrate long-term stateful AI into products.\n",
        "\n",
        "How Does MCP Work?\n",
        "MCP structures context into multiple scopes and uses a standard format to pass data to models. Here's how it breaks down:\n",
        "\n",
        "1. Scopes of Context\n",
        "User Scope: Includes persistent user information (e.g., name, preferences, work history)\n",
        "\n",
        "Thread Scope: Captures session-specific data (e.g., recent conversation, temporary goals)\n",
        "\n",
        "Message Scope: Immediate context for the current input (e.g., a user’s question or prompt)\n",
        "\n",
        "These scopes help the model differentiate what’s permanent, temporary, or immediate, allowing better prioritization.\n",
        "\n",
        "2. Structured Format\n",
        "MCP typically passes data using JSON-like structured objects, clearly labeling each type of context. This makes it easy for LLMs to parse and prioritize.\n",
        "\n",
        "3. Memory Integration\n",
        "When used with model memory features, MCP enables stateful interactions, where the model can remember a user’s past preferences or completed tasks—just like a human assistant.\n",
        "\n",
        "Real-World Applications of MCP\n",
        "MCP is gaining traction across industries, especially where personalized, consistent AI output is crucial.\n",
        "\n",
        "In Technology:\n",
        "AI-powered agents that remember prior conversations and adapt\n",
        "\n",
        "Coding assistants that recall project-specific details\n",
        "\n",
        "Chatbots with persistent user profiles for enhanced customer support\n",
        "\n",
        "In Healthcare:\n",
        "AI tools that remember patient history, medication routines, and appointments\n",
        "\n",
        "Telemedicine assistants that maintain context across sessions\n",
        "\n",
        "In Finance:\n",
        "Personalized financial advisors that track goals over time\n",
        "\n",
        "Chatbots for banking apps that recall transaction patterns and preferences\n",
        "\n",
        "In Education:\n",
        "Tutoring apps that adapt to a student’s learning style and history\n",
        "\n",
        "Homework help bots that remember past questions and feedback\n",
        "\n",
        "Benefits and Challenges\n",
        "✅ Benefits of MCP\n",
        "Better personalization: Tailors responses based on user profiles\n",
        "\n",
        "Reduced prompt length: Saves tokens and compute\n",
        "\n",
        "Improved reasoning: Sustains logical chains across interactions\n",
        "\n",
        "Reusability: Scalable across different apps or user sessions\n",
        "\n",
        "According to OpenAI, models using MCP show a 15–30% improvement in task continuity and user satisfaction in early tests.\n",
        "\n",
        "❌ Challenges of MCP\n",
        "Privacy concerns: Storing persistent user data needs secure handling\n",
        "\n",
        "Implementation complexity: Developers must structure data properly\n",
        "\n",
        "Model compatibility: Not all models or APIs support MCP yet\n",
        "\n",
        "Interesting Facts and Surprising Stats\n",
        "Over 65% of AI agent use cases tested by OpenAI in 2024 relied on MCP or equivalent structured context formats\n",
        "\n",
        "MCP enables \"retrieval-free memory\", meaning models can access context without needing a separate vector database, according to OpenAI documentation\n",
        "\n",
        "Companies using structured context protocols like MCP have seen up to 2x improvements in user retention, per internal OpenAI developer feedback\n",
        "\n",
        "Future Outlook\n",
        "Over the next 5–10 years, Model Context Protocols are expected to become a standard in LLM development. As AI moves from passive tools to active agents, contextual understanding will be foundational. We’ll likely see:\n",
        "\n",
        "Open standards for MCP, similar to OAuth or REST\n",
        "\n",
        "Cross-model interoperability, allowing apps to switch between GPT, Claude, and other LLMs using the same context format\n",
        "\n",
        "Enhanced memory control, where users can manage, erase, or share context across devices\n",
        "\n",
        "This evolution will support more autonomous, trustworthy AI systems, capable of handling multi-step goals and personal tasks.\n",
        "\n",
        "Conclusion\n",
        "So, what is MCP? It’s the emerging protocol that allows AI models to think in context—consistently, intelligently, and securely. As AI becomes more deeply integrated into daily life, MCP is the invisible layer making those interactions feel natural and personalized. Whether you're building the next chatbot or using an AI tutor, understanding how MCP works can help you get better results from language models.\n",
        "\n",
        "Stay ahead by embracing this shift toward context-aware AI—MCP is not just a technical feature; it's a new way of thinking about intelligent systems.\n",
        "\n",
        "Sources:\n",
        "\n",
        "OpenAI Developer Documentation\n",
        "\n",
        "OpenAI Dev Day 2024 Recap\n",
        "\n",
        "GitHub Discussions: MCP Early Access\n",
        "\n",
        "“Personalizing AI with Long-Term Memory,” OpenAI Research Blog\n",
        "\n",
        "The Decoder Tech News'''\n",
        "\n",
        "    # Paraphrase the text\n",
        "    humanized_text = paraphrase_text(ai_text)\n",
        "\n",
        "    if humanized_text:\n",
        "        print(humanized_text)\n",
        "    else:\n",
        "        print(\"Failed to paraphrase text.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uVoq7jdd2t_I",
        "outputId": "98c7e749-2c0b-44db-df47-f92b430bfefd"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Introduction In the ever-evolving world of artificial intelligence, model performance isn't just about size or data—it's about context. As generative AI tools like ChatGPT, Claude and Gemini become central to work and life, protocols like MCP are crucial for better memory, context retention and long-form\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0ZeoEdfB2uBo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WGY0jTeZ2uD6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YsFAu_7k2uG2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "t-cxKgPY2uJC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DViR25zf2uLr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6DrudMaz2uOd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}